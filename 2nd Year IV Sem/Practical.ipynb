{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c2e66-7c56-4ae5-854c-21d059407f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66300d4a-054b-4861-bcd6-671e01277503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the wine dataset\n",
    "wine_data = load_wine()\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11787eaa-6f99-432d-aef7-009319ac4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = pd.DataFrame(data=wine_data.data, columns=wine_data.feature_names)\n",
    "wine_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b809f86-ad56-4a02-8163-3e08e7b471f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding some missing value to dataset for demonstration\n",
    "wine_dataset.loc[3:6 , \"proline\"] = np.nan\n",
    "wine_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05dc22-7242-4132-9b9a-803cf05a7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for the missing values\n",
    "wine_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562518b9-9e96-4763-9971-02ee8c5c23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handlng the missing value\n",
    "# wine_dataset[\"proline\"].fillna(wine_dataset[\"proline\"].mean(), inplace = True)\n",
    "wine_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf56098-bd56-48dc-82b0-68add4e444a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers using z-score\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(wine_dataset))\n",
    "outlier_indices = np.where(z_scores > 3)  # Using a z-score threshold of 3 for outliers\n",
    "wine_dataset_cleaned = wine_dataset.drop(outlier_indices[0])\n",
    "wine_dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbdf30-4e4b-40f3-ada1-e9f915beafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling inconsistent values (if any)\n",
    "# For demonstration, let's assume 'alcohol' values should be in the range [10, 20]\n",
    "wine_dataset_cleaned['alcohol'] = np.where((wine_dataset_cleaned['alcohol'] < 10) | (wine_dataset_cleaned['alcohol'] > 20),\n",
    "                                      np.nan, wine_dataset_cleaned['alcohol'])\n",
    "wine_dataset_cleaned['alcohol'].fillna(wine_dataset_cleaned['alcohol'].mean(), inplace=True)\n",
    "print(wine_dataset_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2849c78-5e6b-4925-b3a3-ff068d35edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after handling outliers and inconsistent values\n",
    "print(\"Missing values after handling outliers and inconsistent values:\")\n",
    "print(wine_dataset_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541bbbd-68f8-4b8a-8c61-9d9960ef7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d35ccd-bc91-41af-ab80-8aa57d4f8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Category\": [\"A\",\"B\",\"A\",\"B\"],\n",
    "           \"Sales\" : [100, 150, 80 , 120],\n",
    "        \"Age\" : [25, 40, 35, 28],\n",
    "        \"Customer\": [10, 15, 8, 12],\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b81ec-94e1-4b30-ae96-836c87421623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "df_aggreated = df.groupby('Category')['Sales'].sum()\n",
    "df_aggreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479dfc6-8887-499d-a6e7-1c24c34dfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Creation\n",
    "df[\"Sales_per_Person\"] = df[\"Sales\"]/df[\"Customer\"]\n",
    "print(\"Sales per person is given: \")\n",
    "print(df[\"Sales_per_Person\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cf0a6-668d-4de2-b801-0d33105fb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarization\n",
    "df[\"Binary Age\"] = (df[\"Age\"]>30).astype(int)\n",
    "print(\"Binary Age\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a9f89-45fa-4344-94c3-094c3f6b5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling\n",
    "sample_df = df.sample(frac = 0.3, random_state = 42)\n",
    "# sample_df = df.sample(2, random_state = 42)\n",
    "\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c353dc86-00af-4df3-8ae6-35214df61aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Age</th>\n",
       "      <th>Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>625</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>150</td>\n",
       "      <td>1600</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>80</td>\n",
       "      <td>1225</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>120</td>\n",
       "      <td>784</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category  Sales   Age  Customer\n",
       "0        A    100   625        10\n",
       "1        B    150  1600        15\n",
       "2        A     80  1225         8\n",
       "3        B    120   784        12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Variable transformation\n",
    "ages = [23,45,67,98,23,49]\n",
    "#Tranforming the variable in squre root\n",
    "transform_ages = [age**0.5 for age in ages]\n",
    "df[\"Age\"] = df[\"Age\"]**2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4997c6-de1c-4acd-84a1-dd3f3cc43822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Create the dataset\n",
    "data = {\n",
    "    \"Category\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"Sales\": [100, 150, 80, 120],\n",
    "    \"Age\": [25, 40, 35, 28],\n",
    "    \"Customer\": [10, 15, 8, 12],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate numerical columns for scaling\n",
    "numerical_columns = [\"Sales\", \"Age\", \"Customer\"]\n",
    "numerical_data = df[numerical_columns]\n",
    "print(numerical_data)\n",
    "\n",
    "# Standardization\n",
    "scaler_standard = StandardScaler()\n",
    "scaled_standard = scaler_standard.fit_transform(numerical_data)\n",
    "df_standardized = pd.DataFrame(scaled_standard, columns=numerical_columns)\n",
    "\n",
    "# Normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_minmax = scaler_minmax.fit_transform(numerical_data)\n",
    "df_normalized = pd.DataFrame(scaled_minmax, columns=numerical_columns)\n",
    "\n",
    "print(\"Standardized Data:\")\n",
    "print(df_standardized)\n",
    "\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(df_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82e067-9af3-4e33-ab5a-36e57427e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "#Aprori Alogrithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a5960-5777-4d74-916d-fd028bdfbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple dataset of transactions\n",
    "dataset = [['milk', 'bread', 'butter'],\n",
    "           ['milk', 'bread'],\n",
    "           ['milk', 'eggs'],\n",
    "           ['bread', 'eggs'],\n",
    "           ['bread', 'butter']]\n",
    "\n",
    "# Encode the dataset\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(df)\n",
    "\n",
    "# Run Apriori algorithm with minimum support of 50% and minimum confidence of 75%\n",
    "frequent_itemsets_a = apriori(df, min_support=0.5, use_colnames=True)\n",
    "rules_a = association_rules(frequent_itemsets_a, metric=\"confidence\", min_threshold=0.75)\n",
    "\n",
    "# Run Apriori algorithm with minimum support of 60% and minimum confidence of 60%\n",
    "frequent_itemsets_b = apriori(df, min_support=0.6, use_colnames=True)\n",
    "rules_b = association_rules(frequent_itemsets_b, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# Print the results\n",
    "print(\"Results for minimum support 50% and minimum confidence 75%:\")\n",
    "print(frequent_itemsets_a)\n",
    "print(rules_a)\n",
    "\n",
    "print(\"\\nResults for minimum support 60% and minimum confidence 60%:\")\n",
    "print(frequent_itemsets_b)\n",
    "print(rules_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87a489-2111-4144-b421-253cd3d2a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#define simple dataset for encoding\n",
    "data = [\n",
    "    ['milk', 'bread', 'butter', 'egg'],\n",
    "    ['milk', 'bread'],\n",
    "           ['milk', 'eggs'],\n",
    "           ['bread', 'eggs'],\n",
    "           ['bread', 'butter']\n",
    "]\n",
    "\n",
    "#Encode the dataset\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns = te.columns_)\n",
    "\n",
    "\n",
    "frequent_itemsets_a = apriori(df, min_support=0.5, use_colnames=True)\n",
    "rules_a = association_rules(frequent_itemsets_a, metric=\"confidence\", min_threshold=0.75)\n",
    "\n",
    "frequent_itemsets_b = apriori(df, min_support=0.6, use_colnames=True)\n",
    "rules_b = association_rules(frequent_itemsets_a, metric = \"confidence\", min_threshold = 0.6)\n",
    "\n",
    "print(\"Result for minimum support as 50% and confidence as 75%\")\n",
    "print(frequent_itemsets_a)\n",
    "print(rules_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf332d-3ebd-4878-b339-999a60513135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_centroids(k, data):\n",
    "    # Randomly initialize centroids\n",
    "    centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n",
    "    return centroids\n",
    "\n",
    "def compute_distances(data, centroids):\n",
    "    # Compute distances between data points and centroids\n",
    "    distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(data, centroids):\n",
    "    # Assign data points to clusters based on closest centroid\n",
    "    distances = compute_distances(data, centroids)\n",
    "    clusters = np.argmin(distances, axis=0)\n",
    "    return clusters\n",
    "\n",
    "def update_centroids(data, clusters, k):\n",
    "    # Update centroids based on assigned clusters\n",
    "    new_centroids = np.array([data[clusters == i].mean(axis=0) for i in range(k)])\n",
    "    return new_centroids\n",
    "\n",
    "def compute_mse(data, centroids, clusters):\n",
    "    # Compute Mean Squared Error (MSE)\n",
    "    distances = compute_distances(data, centroids)\n",
    "    mse = np.mean(np.min(distances**2, axis=0))\n",
    "    return mse\n",
    "\n",
    "def k_means(data, k, max_iterations=100, tol=1e-4):\n",
    "    centroids = initialize_centroids(k, data)\n",
    "    mse_values = []\n",
    "    for _ in range(max_iterations):\n",
    "        clusters = assign_clusters(data, centroids)\n",
    "        new_centroids = update_centroids(data, clusters, k)\n",
    "        mse = compute_mse(data, new_centroids, clusters)\n",
    "        mse_values.append(mse)\n",
    "        if np.abs(np.sum(new_centroids - centroids)) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids, clusters, mse_values\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "data = np.random.rand(100, 2)\n",
    "\n",
    "# Set parameters for K-means\n",
    "k = 3  # Number of clusters\n",
    "max_iterations = 100  # Maximum number of iterations\n",
    "tolerance = 1e-4  # Tolerance for convergence\n",
    "\n",
    "# Run K-means algorithm\n",
    "centroids, clusters, mse_values = k_means(data, k, max_iterations, tolerance)\n",
    "\n",
    "# Plot MSE after each iteration\n",
    "plt.plot(range(1, len(mse_values) + 1), mse_values, marker='o')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs. Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a62f4-d3d6-4705-8f4c-60ea61fb0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Function to compute K-means clustering and plot MSE\n",
    "def compute_kmeans(parameters):\n",
    "    kmeans = KMeans(**parameters)\n",
    "    centroids_history = []\n",
    "    mse_history = []\n",
    "    for iteration in range(parameters['max_iter']):\n",
    "        kmeans.fit(X)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        labels = kmeans.labels_\n",
    "        mse = mean_squared_error(X, centroids[labels])\n",
    "        centroids_history.append(centroids)\n",
    "        mse_history.append(mse)\n",
    "    return mse_history, centroids_history\n",
    "\n",
    "# Define parameters for K-means clustering\n",
    "parameters_list = [\n",
    "    {'n_clusters': 2, 'init': 'random', 'max_iter': 10},\n",
    "    {'n_clusters': 3, 'init': 'k-means++', 'max_iter': 10},\n",
    "    {'n_clusters': 4, 'init': 'random', 'max_iter': 10},\n",
    "    {'n_clusters': 4, 'init': 'k-means++', 'max_iter': 10},\n",
    "]\n",
    "\n",
    "# Compute K-means clustering for each set of parameters and plot MSE\n",
    "plt.figure(figsize=(10, 6))\n",
    "for params in parameters_list:\n",
    "    mse_history, _ = compute_kmeans(params)\n",
    "    plt.plot(range(1, params['max_iter'] + 1), mse_history, label=f\"Clusters: {params['n_clusters']}, Init: {params['init']}\")\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('K-means Clustering Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9e3e8-7004-45e3-a6b7-d93c107f2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load datasets\n",
    "iris_data = load_iris()\n",
    "breast_cancer_data = load_breast_cancer()\n",
    "\n",
    "# Function to build and evaluate classifiers\n",
    "def evaluate_classifiers(data, target, test_size):\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=test_size, random_state=0)\n",
    "    \n",
    "    # Initialize classifiers\n",
    "    classifiers = {\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=0)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        # Train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[name] = accuracy\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate classifiers on Iris dataset with different training/test splits\n",
    "print(\"Iris Dataset:\")\n",
    "for test_size in [0.25, 0.33]:\n",
    "    results = evaluate_classifiers(iris_data.data, iris_data.target, test_size)\n",
    "    print(f\"Test Size {test_size}: {results}\")\n",
    "\n",
    "# Evaluate classifiers on Breast Cancer dataset with different training/test splits\n",
    "print(\"\\nBreast Cancer Dataset:\")\n",
    "for test_size in [0.25, 0.33]:\n",
    "    results = evaluate_classifiers(breast_cancer_data.data, breast_cancer_data.target, test_size)\n",
    "    print(f\"Test Size {test_size}: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec29794-cf3c-4555-9729-2ff512d39df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load datasets\n",
    "iris_data = load_iris()\n",
    "breast_cancer_data = load_breast_cancer()\n",
    "\n",
    "# Scale the data to a standard format\n",
    "scaler = StandardScaler()\n",
    "iris_data_scaled = scaler.fit_transform(iris_data.data)\n",
    "bc_data_scaled = scaler.fit_transform(breast_cancer_data.data)\n",
    "\n",
    "# Function to build and evaluate classifiers with different training set selection methods\n",
    "def evaluate_classifiers(data, target, train_method):\n",
    "    results = {}\n",
    "    \n",
    "    if train_method == 'holdout':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=0)\n",
    "        results['Holdout'] = evaluate_accuracy(X_train, X_test, y_train, y_test)\n",
    "    elif train_method == 'random_sub':\n",
    "        for _ in range(3):  # Perform 3 random subsamplings\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=None)\n",
    "            results[f'Random Subsampling {_+1}'] = evaluate_accuracy(X_train, X_test, y_train, y_test)\n",
    "    elif train_method == 'cross_val':\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "            X_train, X_test = data[train_index], data[test_index]\n",
    "            y_train, y_test = target[train_index], target[test_index]\n",
    "            results[f'Cross-Validation Fold {fold+1}'] = evaluate_accuracy(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to evaluate accuracy of classifiers\n",
    "def evaluate_accuracy(X_train, X_test, y_train, y_test):\n",
    "    classifiers = {\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=0)\n",
    "    }\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        pipeline = make_pipeline(StandardScaler(), clf)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[name] = accuracy\n",
    "    return results\n",
    "\n",
    "# Evaluate classifiers using different training set selection methods on Iris dataset\n",
    "print(\"Iris Dataset:\")\n",
    "for train_method in ['holdout', 'random_sub', 'cross_val']:\n",
    "    results = evaluate_classifiers(iris_data_scaled, iris_data.target, train_method)\n",
    "    print(f\"Training Method: {train_method}\")\n",
    "    print(results)\n",
    "\n",
    "# Evaluate classifiers using different training set selection methods on Breast Cancer dataset\n",
    "print(\"\\nBreast Cancer Dataset:\")\n",
    "for train_method in ['holdout', 'random_sub', 'cross_val']:\n",
    "    results = evaluate_classifiers(bc_data_scaled, breast_cancer_data.target, train_method)\n",
    "    print(f\"Training Method: {train_method}\")\n",
    "    print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9afab08-8688-42ce-b039-18e1b8cdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Sales  Age  Customer  Sales/Person  B age\n",
      "0        A    100   25        10          10.0      0\n",
      "1        B    150   40        15          10.0      1\n",
      "2        A     80   35         8          10.0      1\n",
      "3        B    120   28        12          10.0      0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data = {\n",
    "    \"Category\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"Sales\": [100, 150, 80, 120],\n",
    "    \"Age\": [25, 40, 35, 28],\n",
    "    \"Customer\": [10, 15, 8, 12],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_aggreated = df.groupby(\"Category\")[\"Sales\"].sum()\n",
    "\n",
    "df[\"Sales/Person\"] = df[\"Sales\"]/df[\"Customer\"]\n",
    "df[\"B age\"] = (df[\"Age\"] > 30).astype(int)\n",
    "print(df)\n",
    "sdf = df.sample(frac=0.3, random_state=42)\n",
    "\n",
    "numerical_columns = [\"Sales\", \"Age\", \"Customer\"]\n",
    "numerical_data = df[numerical_columns]\n",
    "s_std = StandardScaler().fit_transform(numerical_data)\n",
    "s_scaler = MinMaxScaler().fit_transform(numerical_data)\n",
    "\n",
    "df_std = pd.DataFrame(s_std, columns=numerical_columns)\n",
    "df_std = pd.DataFrame(s_scaler, columns=numerical_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
